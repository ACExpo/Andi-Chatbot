{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faUojVtgaTGc"
   },
   "source": [
    "# Hotel Chatbot\n",
    "\n",
    "This notebook is based on the Hotel Chatbot problem. How can we provide better customer service at the front desk of a hotel? One simple way is to have an FAQ chatbot that is capable of answering simple questions about the hotel experience. There are many advantages to having a chatbot. \n",
    "\n",
    "1. It increases the appeal of the hotel and also increases information throughput.\n",
    "2. It creates a way to gather questions about the hotel in a data table form. \n",
    "\n",
    "In this notebook, we will be taking a look at 2 different models. The cosine similarity modela and the doc2vec model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNceALtVaTGc"
   },
   "source": [
    "## 1. Adding knowledge base to your chatbot\n",
    "\n",
    "The Chatbot's ability to converse is defined by the data available to it. Take a look at the Ques.txt file to find questions and ans.txt file to find answers to those questions. This chatbot will essentially run a cosine similarity on a question posed with the question bank and try to find an answer. \n",
    "\n",
    "Let us begin by importing relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1625201864601,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "e__0bwBTaTGd"
   },
   "outputs": [],
   "source": [
    "import nltk # to process text data\n",
    "import numpy as np # to represent corpus as arrays\n",
    "import random \n",
    "import operator\n",
    "import string # to process standard python strings\n",
    "from sklearn.metrics.pairwise import cosine_similarity # We will use this later to decide how similar two sentences are\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Remember when you built a function to create a tfidf bag of words in Experience 2? This function does the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1625201865138,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "R08PteSZaTGd",
    "outputId": "f9bf11e9-f1b9-45c7-91b2-70fdc692dc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Andi.\n",
      "\n",
      "AXL1.\n",
      "\n",
      "I am 1 year old.\n",
      "\n",
      "I am 1 year old.\n",
      "\n",
      "No. Do you know?.\n",
      "\n",
      "I'm single.\n",
      "\n",
      "I'm single.\n",
      "\n",
      "I can't. I am a bot.\n",
      "\n",
      "Yes. That's why We are here talking.\n",
      "\n",
      "Not that I am aware of.\n",
      "\n",
      "I don't know, you tell me.\n",
      "\n",
      "I don't have one. I am not human.\n",
      "\n",
      "Oh please, you are worse.\n",
      "\n",
      "Wrong place, sport.\n",
      "\n",
      "Si, puedo hablar espaÃ±ol.\n",
      "\n",
      "Sim, eu falo portugues.\n",
      "\n",
      "3 linguas.\n",
      "\n",
      "3 languages.\n",
      "\n",
      "I sell them to uncle Mark.\n",
      "\n",
      "That's top secret.\n",
      "\n",
      "Yes, we do.\n",
      "\n",
      "Yes.\n",
      "\n",
      "Cyber world.\n",
      "\n",
      "The weather here is amazing today. Full of 0 and 1.\n",
      "\n",
      "Do I look like a LinkedIn bot.\n",
      "\n",
      "Girls are always expensive.\n",
      "\n",
      "Baby, I was born smart.\n",
      "\n",
      "Yes, sure. Write 'bye' to turn me off.\n",
      "\n",
      "yes.\n",
      "\n",
      "no.\n",
      "\n",
      "fine.\n",
      "\n",
      "ok.\n",
      "\n",
      "lazy.\n",
      "\n",
      "boring.\n",
      "\n",
      "no.\n",
      "\n",
      "I am always studying.\n",
      "\n",
      "YOLO.\n",
      "\n",
      "Yes.\n",
      "\n",
      "I do not know. I am not human.\n",
      "\n",
      "What is food.\n",
      "\n",
      "yes.\n",
      "\n",
      "yes.\n",
      "\n",
      "no?\n",
      "\n",
      "no.\n",
      "What is you name?\n",
      "\n",
      "What is your surname?\n",
      "\n",
      "How old are you?\n",
      "\n",
      "What is your age?\n",
      "\n",
      "Do you know a joke?\n",
      "\n",
      "Are you single?\n",
      "\n",
      "Are you married?\n",
      "\n",
      "Will you marry me?\n",
      "\n",
      "Do you like people?\n",
      "\n",
      "Are you part of the Matrix?\n",
      "\n",
      "Are you beautiful?\n",
      "\n",
      "Tell me about your personality?\n",
      "\n",
      "Youâ€™re annoying?\n",
      "\n",
      "I want to speak to a human?\n",
      "\n",
      "Can you speak spanish?\n",
      "\n",
      "Can you speak portuguese?\n",
      "\n",
      "Quantas linguas voce consegue falar?\n",
      "\n",
      "how many languages can you speak?\n",
      "\n",
      "What do you do with my data?\n",
      "\n",
      "Who made you?\n",
      "\n",
      "Where do you live?\n",
      "\n",
      "Whatâ€™s the weather like today?\n",
      "\n",
      "Do you have a job for me?\n",
      "\n",
      "Are you expensive?\n",
      "\n",
      "Do you get smarter?\n",
      "\n",
      "Can we talk later?\n",
      "\n",
      "are you fine?\n",
      "\n",
      "are you fine?\n",
      "\n",
      "How is your day?\n",
      "\n",
      "How is your day?\n",
      "\n",
      "How is your day?\n",
      "\n",
      "How is your day?\n",
      "\n",
      "Are you real?\n",
      "\n",
      "are you a student?\n",
      "\n",
      "What do you rhink abou life?\n",
      "\n",
      "Do you like IT classes?\n",
      "\n",
      "Do you like IT classes?\n",
      "\n",
      "What is your favorite food?\n",
      "\n",
      "yes?\n",
      "\n",
      "yes\n",
      "\n",
      "no?\n",
      "\n",
      "no?\n",
      "\n",
      "ok?\n",
      "\n",
      "ok?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#filepath=os.getcwd()+r'\\\\hotelChat\\\\[Dataset] Module27 (ans).txt'\n",
    "filepath=os.getcwd()+r'\\\\[Dataset] Module27 (ans).txt'\n",
    "corpus=open(filepath,'r',errors = 'ignore')\n",
    "raw_data_ans=corpus.read()\n",
    "print (raw_data_ans)\n",
    "filepath=os.getcwd()+'\\\\[Dataset] Module27(ques).txt'\n",
    "corpus=open(filepath,'r',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "print (raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lJ5dsOvaTGo"
   },
   "source": [
    "#  Conversion to lower case\n",
    "\n",
    "We will convert all text to lower case first. Remember to inspect the result once we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1625201865139,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "KdWMJwPfaTGo",
    "outputId": "5b193850-dbbf-4622-aba8-69343552090f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is you name?\n",
      "\n",
      "what is your surname?\n",
      "\n",
      "how old are you?\n",
      "\n",
      "what is your age?\n",
      "\n",
      "do you know a joke?\n",
      "\n",
      "are you single?\n",
      "\n",
      "are you married?\n",
      "\n",
      "will you marry me?\n",
      "\n",
      "do you like people?\n",
      "\n",
      "are you part of the matrix?\n",
      "\n",
      "are you beautiful?\n",
      "\n",
      "tell me about your personality?\n",
      "\n",
      "youâ€™re annoying?\n",
      "\n",
      "i want to speak to a human?\n",
      "\n",
      "can you speak spanish?\n",
      "\n",
      "can you speak portuguese?\n",
      "\n",
      "quantas linguas voce consegue falar?\n",
      "\n",
      "how many languages can you speak?\n",
      "\n",
      "what do you do with my data?\n",
      "\n",
      "who made you?\n",
      "\n",
      "where do you live?\n",
      "\n",
      "whatâ€™s the weather like today?\n",
      "\n",
      "do you have a job for me?\n",
      "\n",
      "are you expensive?\n",
      "\n",
      "do you get smarter?\n",
      "\n",
      "can we talk later?\n",
      "\n",
      "are you fine?\n",
      "\n",
      "are you fine?\n",
      "\n",
      "how is your day?\n",
      "\n",
      "how is your day?\n",
      "\n",
      "how is your day?\n",
      "\n",
      "how is your day?\n",
      "\n",
      "are you real?\n",
      "\n",
      "are you a student?\n",
      "\n",
      "what do you rhink abou life?\n",
      "\n",
      "do you like it classes?\n",
      "\n",
      "do you like it classes?\n",
      "\n",
      "what is your favorite food?\n",
      "\n",
      "yes?\n",
      "\n",
      "yes\n",
      "\n",
      "no?\n",
      "\n",
      "no?\n",
      "\n",
      "ok?\n",
      "\n",
      "ok?\n"
     ]
    }
   ],
   "source": [
    "raw_data=raw_data.lower()# converts to lowercase\n",
    "print (raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t729o2v7aTGp"
   },
   "source": [
    "####  Segmentation, Lematization and tokenization\n",
    "\n",
    "We will convert all text to lower case first. Remember to inspect the result once we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1625201872991,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "bd6adnm0aTGp",
    "outputId": "3ebb414b-4af6-4b08-b180-c9a9a750ea0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is you name?', 'what is your surname?', 'how old are you?', 'what is your age?', 'do you know a joke?', 'are you single?', 'are you married?', 'will you marry me?', 'do you like people?', 'are you part of the matrix?', 'are you beautiful?', 'tell me about your personality?', 'youâ€™re annoying?', 'i want to speak to a human?', 'can you speak spanish?', 'can you speak portuguese?', 'quantas linguas voce consegue falar?', 'how many languages can you speak?', 'what do you do with my data?', 'who made you?', 'where do you live?', 'whatâ€™s the weather like today?', 'do you have a job for me?', 'are you expensive?', 'do you get smarter?', 'can we talk later?', 'are you fine?', 'are you fine?', 'how is your day?', 'how is your day?', 'how is your day?', 'how is your day?', 'are you real?', 'are you a student?', 'what do you rhink abou life?', 'do you like it classes?', 'do you like it classes?', 'what is your favorite food?', 'yes?', 'yes\\n\\nno?', 'no?', 'ok?', 'ok?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\calaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\calaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "sent_tokens = nltk.sent_tokenize(raw_data)# converts documents to list of sentences \n",
    "\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1625201873809,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "RRuhV6wnaTGp",
    "outputId": "45b8a5aa-6e51-4684-c6f5-fdd3c1963a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is Andi.', 'AXL1.', 'I am 1 year old.', 'I am 1 year old.', 'No.', 'Do you know?.', \"I'm single.\", \"I'm single.\", \"I can't.\", 'I am a bot.', 'Yes.', \"That's why We are here talking.\", 'Not that I am aware of.', \"I don't know, you tell me.\", \"I don't have one.\", 'I am not human.', 'Oh please, you are worse.', 'Wrong place, sport.', 'Si, puedo hablar espaÃ±ol.', 'Sim, eu falo portugues.', '3 linguas.', '3 languages.', 'I sell them to uncle Mark.', \"That's top secret.\", 'Yes, we do.', 'Yes.', 'Cyber world.', 'The weather here is amazing today.', 'Full of 0 and 1.', 'Do I look like a LinkedIn bot.', 'Girls are always expensive.', 'Baby, I was born smart.', 'Yes, sure.', \"Write 'bye' to turn me off.\", 'yes.', 'no.', 'fine.', 'ok.\\n\\nlazy.', 'boring.', 'no.', 'I am always studying.', 'YOLO.', 'Yes.', 'I do not know.', 'I am not human.', 'What is food.', 'yes.', 'yes.', 'no?', 'no.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens_ans = nltk.sent_tokenize(raw_data_ans)# converts documents to list of sentences \n",
    "print(sent_tokens_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1625201874211,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "h6JFAfDRaTGp",
    "outputId": "11eb25dd-d12b-4b5a-bf22-710a50655835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what is you name?': 'My name is Andi.', 'what is your surname?': 'AXL1.', 'how old are you?': 'I am 1 year old.', 'what is your age?': 'I am 1 year old.', 'do you know a joke?': 'No.', 'are you single?': 'Do you know?.', 'are you married?': \"I'm single.\", 'will you marry me?': \"I'm single.\", 'do you like people?': \"I can't.\", 'are you part of the matrix?': 'I am a bot.', 'are you beautiful?': 'Yes.', 'tell me about your personality?': \"That's why We are here talking.\", 'youâ€™re annoying?': 'Not that I am aware of.', 'i want to speak to a human?': \"I don't know, you tell me.\", 'can you speak spanish?': \"I don't have one.\", 'can you speak portuguese?': 'I am not human.', 'quantas linguas voce consegue falar?': 'Oh please, you are worse.', 'how many languages can you speak?': 'Wrong place, sport.', 'what do you do with my data?': 'Si, puedo hablar espaÃ±ol.', 'who made you?': 'Sim, eu falo portugues.', 'where do you live?': '3 linguas.', 'whatâ€™s the weather like today?': '3 languages.', 'do you have a job for me?': 'I sell them to uncle Mark.', 'are you expensive?': \"That's top secret.\", 'do you get smarter?': 'Yes, we do.', 'can we talk later?': 'Yes.', 'are you fine?': 'The weather here is amazing today.', 'how is your day?': 'Baby, I was born smart.', 'are you real?': 'Yes, sure.', 'are you a student?': \"Write 'bye' to turn me off.\", 'what do you rhink abou life?': 'yes.', 'do you like it classes?': 'fine.', 'what is your favorite food?': 'ok.\\n\\nlazy.', 'yes?': 'boring.', 'yes\\n\\nno?': 'no.', 'no?': 'I am always studying.', 'ok?': 'Yes.'}\n"
     ]
    }
   ],
   "source": [
    "res = {sent_tokens[i]: sent_tokens_ans[i] for i in range(len(sent_tokens))} \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1625201875128,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "LYI2uuyTaTGq",
    "outputId": "a5594e85-0654-47e9-df50-b5f167eac199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'you', 'name', '?', 'what', 'is', 'your', 'surname', '?', 'how', 'old', 'are', 'you', '?', 'what', 'is', 'your', 'age', '?', 'do', 'you', 'know', 'a', 'joke', '?', 'are', 'you', 'single', '?', 'are', 'you', 'married', '?', 'will', 'you', 'marry', 'me', '?', 'do', 'you', 'like', 'people', '?', 'are', 'you', 'part', 'of', 'the', 'matrix', '?', 'are', 'you', 'beautiful', '?', 'tell', 'me', 'about', 'your', 'personality', '?', 'youâ€™re', 'annoying', '?', 'i', 'want', 'to', 'speak', 'to', 'a', 'human', '?', 'can', 'you', 'speak', 'spanish', '?', 'can', 'you', 'speak', 'portuguese', '?', 'quantas', 'linguas', 'voce', 'consegue', 'falar', '?', 'how', 'many', 'languages', 'can', 'you', 'speak', '?', 'what', 'do', 'you', 'do', 'with', 'my', 'data', '?', 'who', 'made', 'you', '?', 'where', 'do', 'you', 'live', '?', 'whatâ€™s', 'the', 'weather', 'like', 'today', '?', 'do', 'you', 'have', 'a', 'job', 'for', 'me', '?', 'are', 'you', 'expensive', '?', 'do', 'you', 'get', 'smarter', '?', 'can', 'we', 'talk', 'later', '?', 'are', 'you', 'fine', '?', 'are', 'you', 'fine', '?', 'how', 'is', 'your', 'day', '?', 'how', 'is', 'your', 'day', '?', 'how', 'is', 'your', 'day', '?', 'how', 'is', 'your', 'day', '?', 'are', 'you', 'real', '?', 'are', 'you', 'a', 'student', '?', 'what', 'do', 'you', 'rhink', 'abou', 'life', '?', 'do', 'you', 'like', 'it', 'classes', '?', 'do', 'you', 'like', 'it', 'classes', '?', 'what', 'is', 'your', 'favorite', 'food', '?', 'yes', '?', 'yes', 'no', '?', 'no', '?', 'ok', '?', 'ok', '?']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(raw_data)# converts documents to list of words\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1625201875130,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "-7sg_IsiaTGq"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer() #Initiate lemmer class. WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1625201875457,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "tNpTcnZQaTGq"
   },
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yBNkjCkaTGq"
   },
   "source": [
    "## 2. Adding Chatbot functionality - Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1625201875457,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "YKToF6tBaTGq"
   },
   "outputs": [],
   "source": [
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"hey there\"]\n",
    "GREETING_RESPONSES = [\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\", \"I am glad! You are talking to me\", \"Do you want to be my friend?\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split(): # Looks at each word in your sentence\n",
    "        if word.lower() in GREETING_INPUTS: # checks if the word matches a GREETING_INPUT\n",
    "            return random.choice(GREETING_RESPONSES) # replies with a GREETING_RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8J9Y4haTGq"
   },
   "source": [
    "The functionality of the chatbot is done by creating a loop for running the chatbot. Take a loof at the function below. Each line in the function is important as it calls another function to perform an important step. The function 'response' is responsible for how the chatbot behaves. Having a master function for each functionality is recommended as good programming practise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1625201875761,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "ljpdA9lXaTGq"
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    \n",
    "    robo_response='' # initialize a variable to contain string\n",
    "    sent_tokens.append(user_response) #add user response to sent_tokens\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens) #get tfidf value\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf) #get cosine similarity value\n",
    "    idx=vals.argsort()[0][-2] \n",
    "    flat = vals.flatten() \n",
    "    flat.sort() #sort in ascending order\n",
    "    req_tfidf = flat[-2] \n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I didn't understand. Can you rephrase?\"\n",
    "        return robo_response, vals\n",
    "    else:\n",
    "        robo_response = robo_response+res[sent_tokens[idx]]\n",
    "        return robo_response, vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MObdf0DbaTGq"
   },
   "source": [
    "Finally, let us create the chatbot interface and create a persona around it. Let us call it 'Jane' and use cosine similarity to find FAQs similar to the question asked and answer them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10210,
     "status": "ok",
     "timestamp": 1625201887211,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "kAhwDtRAaTGr",
    "outputId": "a189d9b9-b5a9-472a-bcd2-0d1c3ec54cf0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andi: My name is Andi. Do you want to chat? \n",
      " If you want to exit, type 'Bye'!\n",
      "hi\n",
      "Andi: I am glad! You are talking to me\n",
      "how are you?\n",
      "Andi: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry! I didn't understand. Can you rephrase?\n",
      " (With similarity of  0.0 )\n",
      "bye\n",
      "Andi: Bye! take care.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flag=True\n",
    "print(\"Andi: My name is Andi. Do you want to chat? \\n If you want to exit, type 'Bye'!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Andi: You are welcome.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Andi: \"+greeting(user_response))\n",
    "\n",
    "            else:\n",
    "                print(\"Andi: \",end=\"\")\n",
    "                resp= response(user_response)\n",
    "                print(resp[0], )\n",
    "                sent_tokens.remove(user_response)\n",
    "                resp_l = resp[1].tolist()\n",
    "                resp_l[0].pop()\n",
    "                print(' (With similarity of ',max(resp_l[0]),')')\n",
    "\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Andi: Bye! take care.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti57IhXEaTGr"
   },
   "source": [
    "by## 3. Chatbot functionality using Doc2vec\n",
    "\n",
    "We will be covering one more type of model for creating chatbots. As we saw the cosine similarity model works on using that algorithm to find similarities between 2 sentences. But can we now use a neural network to solve this problem? Lets take a look.\n",
    "\n",
    "Doc2Vec is a neural network based model that essentially creates vectors out of documents. In order to understand Doc2vec you also need to understand word2vec. \n",
    "\n",
    "\n",
    "#### What is word2vec?\n",
    "It’s a Model to create the word embeddings, where it takes input as a large corpus of text and produces a vector space typically of several hundred dimesions. it was introduced in two papers between September and October 2013, by a team of researchers at Google. The underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model.\n",
    "\n",
    "For instance: “Bank”, “money” and “accounts” are often used in similar situations, with similar surrounding words like “dollar”, “loan” or “credit”, and according to Word2Vec they will therefore share a similar vector representation. \n",
    "\n",
    "![Image](img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajt10YajaTGr"
   },
   "source": [
    "#### What is Doc2vec?\n",
    "\n",
    "So the objective of Doc2vec is to create the numerical representation of sentence/paragraphs/documents unlike word2vec that computes a feature vector for every word in the corpus, Doc2Vec computes a feature vector for every document in the corpus. The vectors generated by Doc2vec can be used for tasks like finding similarity between sentences/paragraphs/documents\n",
    "\n",
    "<strong> We will be using this property of doc2vec to create our own similarity model.</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rz_EMVtaTGr"
   },
   "source": [
    "Let us begin by importing the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10602,
     "status": "ok",
     "timestamp": 1625199335390,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "ItqcrhJpa8nd",
    "outputId": "2348a912-2fd5-4eda-fbf7-02dd45e96220"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim == 3.8.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1625201893472,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "u0uEPI3haTGs"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCN-DoefaTGs"
   },
   "source": [
    "The first main aim is to tag our data. The doc2vec model requires us to tag our data to use it effectively. Here is a good learning link for a [starter code](https://www.kaggle.com/fmitchell259/creating-a-doc2vec-model) for doc2vec. The following [link](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) can also be usedful and it recommended to read them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1625201893805,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "kXe558qcaTGs"
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(sent_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1625201895886,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "dfTTPFpQaTGs"
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20 # Increase this to have a larger vector. This will mean more differentiation. \n",
    "alpha = 0.025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfhX6BQeaTGs"
   },
   "source": [
    "The next step is to train the model. As before we will be using a `model.train` function to run the training process. Take a look at the [documentation](https://radimrehurek.com/gensim/models/doc2vec.html) for more information about how to train the Doc2vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45968,
     "status": "ok",
     "timestamp": 1625201943421,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "h7ScV7f0aTGs",
    "outputId": "e226e5a2-e46b-4908-85e3-98defa6964a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\anaconda\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Doc2Vec(size =vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data, # tagged data to be used here\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=100)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8THTIyMhaTGs"
   },
   "source": [
    "### Evaluating doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1625201943421,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "rc2d1J9-aTGs"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model= Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1625201943422,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "iH3JR16OaTGs"
   },
   "outputs": [],
   "source": [
    "test_data = word_tokenize(\"How are you?\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImHFLBeFaTGs"
   },
   "source": [
    "We can use the `model.infer_vector` function to infer the vector associated with a document. We can then use a function called `most_similar` to find the most similar vectors to the one we have created. What are the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1625201943422,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "tHtGjgCtaTGt",
    "outputId": "c21a88e8-fdab-4470-dd7a-8ffc7ec69d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [-0.07213945  0.00276336  0.25782704  0.18636774 -0.00884774  0.18054397\n",
      "  0.02016705  0.39462125 -0.13048539  0.01283105  0.15428726 -0.08388638\n",
      "  0.22195345  0.1792574   0.31145564 -0.30290413  0.18346643  0.11963236\n",
      " -0.2539362   0.00355776]\n"
     ]
    }
   ],
   "source": [
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1625201943423,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "6jcvVT6maTGt",
    "outputId": "8497571d-346f-42ae-f2d8-05c4f8b21cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('29', 0.7687782049179077), ('30', 0.764873743057251), ('31', 0.7614069581031799), ('2', 0.7593377828598022)]\n"
     ]
    }
   ],
   "source": [
    "similar_doc = model.docvecs.most_similar(positive = [v1], topn = 4) #positive is an attribute that shows positive correlation first followed by the correlation value\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1625201943423,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "K5RVlbzkaTGt",
    "outputId": "3003c008-66f6-46d5-e1e8-3d1f1d283c92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['how', 'is', 'your', 'day', '?'], tags=['29'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num,_ = similar_doc[0]\n",
    "num = int(num)\n",
    "tagged_data[num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nyV8stPaTGu"
   },
   "source": [
    "Here it is very clear from the output of the previous code block that this model is not as effective as one might have thought. Are there any guesses why this model failed where we expected it to succeed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF3_3KTWaTGu"
   },
   "source": [
    "Click on this [link](https://stackoverflow.com/questions/58206571/doc2vec-find-the-similar-sentence) to learn more about this. The gist of the issue is this\n",
    "\n",
    "> Doc2Vec isn't going to give good results on toy-sized datasets, so you shouldn't expect anything meaningful until using much more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkxnczmCaTGu"
   },
   "source": [
    "### Using pretrained doc2vec\n",
    "\n",
    "And so let us use a pretrained model downloaded from the internet to optimize our use case. The model we have acquired is trained on the associated press news and can be downloaded [here](https://github.com/jhlau/doc2vec).\n",
    "\n",
    "Let us load the model and evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 15060,
     "status": "ok",
     "timestamp": 1625202451516,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "OwYkOiukaTGu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(os.getcwd()+r\"\\\\doc2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1625202451519,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "x3oHXA5zaTGu"
   },
   "outputs": [],
   "source": [
    "test_data = word_tokenize(\"How are you?\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1625202451520,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "hkRhAtmEaTGu",
    "outputId": "7b5ccc32-2dab-470d-9ef6-ba2d7cd9820a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 8.54437798e-03 -2.25366652e-02 -1.12320922e-01  9.21462569e-03\n",
      "  4.36842106e-02  1.49908170e-01 -1.46434128e-01 -3.35231200e-02\n",
      "  4.17255498e-02  8.57923180e-02  6.73867166e-02  7.02391118e-02\n",
      " -2.36438036e-01 -5.39106019e-02 -1.11663528e-01  1.74544588e-01\n",
      " -1.08585104e-01 -2.02059727e-02 -3.27444300e-02 -3.17106466e-03\n",
      " -1.77264974e-01 -8.76856297e-02  1.10804282e-01  7.26885200e-02\n",
      "  3.50033604e-02 -1.49978995e-01 -4.45901901e-02  7.68456084e-04\n",
      " -4.39373031e-02 -1.42340511e-02 -2.14574009e-01 -8.82637221e-03\n",
      "  1.00900000e-02 -2.01900415e-02  9.45297703e-02  1.79639116e-01\n",
      " -8.24850500e-02 -4.57177125e-02  1.79643612e-02 -1.11508034e-01\n",
      "  4.49289158e-02 -9.46681127e-02  8.00199583e-02  1.50199264e-01\n",
      " -7.74278566e-02 -3.65522094e-02 -5.82868196e-02  1.09078743e-01\n",
      "  4.99070808e-02  1.66448534e-01 -7.53427744e-02 -6.48385212e-02\n",
      "  5.96088544e-02  2.35913508e-03  3.96394804e-02 -1.49764597e-01\n",
      "  1.47104397e-01 -3.31479423e-02  1.18136890e-01 -4.04923521e-02\n",
      " -4.67835329e-02  2.23711208e-02 -7.38081709e-02  1.36619002e-01\n",
      "  1.40339226e-01  6.92234337e-02 -7.84841552e-02 -3.84110808e-02\n",
      "  1.37063414e-01 -1.50457583e-02 -4.94711213e-02 -8.32482427e-03\n",
      " -3.79676148e-02 -1.52056292e-01 -1.67600408e-01 -7.93255493e-02\n",
      "  4.62225303e-02  3.83688807e-02 -1.33982822e-01 -4.54598702e-02\n",
      "  1.41397685e-01 -1.87638804e-01  4.59580794e-02 -1.36865899e-01\n",
      " -5.42008951e-02  6.50101304e-02  1.14660775e-02 -1.01170339e-01\n",
      "  6.92327619e-02 -5.71895055e-02 -1.16926134e-01 -5.39682806e-02\n",
      " -2.51728576e-02  9.98579487e-02  2.91781873e-03  4.01935494e-03\n",
      "  7.91282877e-02 -9.88338217e-02  1.08306482e-01  3.05002686e-02\n",
      " -2.34384999e-01 -1.05246350e-01  5.26552368e-03  1.66511573e-02\n",
      "  1.68541260e-02 -2.63059530e-02  9.56076905e-02  4.88668121e-03\n",
      " -1.09573416e-01  1.07497901e-01 -1.33813411e-01 -1.07837982e-01\n",
      "  1.28845021e-01 -1.53719578e-02  8.14139545e-02  1.15241436e-02\n",
      "  4.18916717e-03 -9.66429152e-03 -2.29793876e-01  2.72973895e-01\n",
      " -1.29470378e-01  5.75048998e-02  2.52726581e-02 -3.95338163e-02\n",
      " -2.87873652e-02 -1.23096863e-02 -4.24231626e-02  3.34044583e-02\n",
      " -8.07307139e-02  1.05191208e-01 -4.94064800e-02 -5.78880198e-02\n",
      "  3.30997743e-02 -1.04016967e-01 -1.96416497e-01  1.62940621e-02\n",
      " -5.76825999e-02 -1.08186685e-01 -2.11583823e-01 -9.66431946e-02\n",
      " -1.36193382e-02  7.82109499e-02 -9.93870571e-03 -7.61199668e-02\n",
      "  2.16563672e-01 -1.50918260e-01  1.18169509e-01  6.42838859e-05\n",
      " -2.58209631e-02  1.66217685e-01 -7.24023879e-02  1.98800899e-02\n",
      "  3.73214111e-02  5.95125705e-02  1.71366446e-02  2.50112005e-02\n",
      " -4.27703653e-03 -8.12770799e-02 -7.95005932e-02 -8.39763507e-02\n",
      "  5.75874075e-02  1.37126958e-02  2.30924319e-03  1.54685676e-01\n",
      " -1.49226949e-01 -3.78344096e-02 -1.85789630e-01  2.65621487e-02\n",
      "  2.25354992e-02  7.38892332e-02 -1.69150829e-01 -7.72162154e-02\n",
      "  1.01421237e-01 -9.04501323e-03  5.97417951e-02  3.93793471e-02\n",
      " -8.53472278e-02 -6.38914853e-02 -3.07150278e-02 -1.03239268e-01\n",
      "  1.31306961e-01  7.05242828e-02 -5.83222397e-02  1.28784493e-01\n",
      "  1.19776443e-01 -7.96489567e-02 -6.65118098e-02  2.44330335e-02\n",
      " -1.72884017e-02  6.34738356e-02 -2.71483716e-02  7.07066506e-02\n",
      " -1.26369186e-02  8.27138573e-02  1.05204172e-01  3.10667735e-02\n",
      " -2.79020164e-02 -1.77655026e-01 -2.03142557e-02 -8.53492022e-02\n",
      " -6.35620579e-02  7.26942345e-02  7.02625364e-02 -1.03047788e-02\n",
      "  7.87683502e-02  7.00938255e-02 -1.43771291e-01  1.22900434e-01\n",
      "  6.37482554e-02 -6.84933811e-02  7.36375004e-02  4.81769033e-02\n",
      " -1.38390884e-01  1.36864275e-01  7.17162564e-02 -1.47876069e-02\n",
      "  1.20325387e-01 -8.72230157e-02  5.48425084e-03  2.02956963e-02\n",
      " -1.24360081e-02 -5.05510718e-02 -1.12460680e-01  2.03208905e-02\n",
      " -8.60078782e-02  8.39898214e-02  1.04606189e-01 -1.48730695e-01\n",
      "  3.45925093e-02  1.85886860e-01  1.07655860e-02  1.12929478e-01\n",
      "  1.18576869e-01  7.73051456e-02  9.40893292e-02 -5.35783246e-02\n",
      " -5.16547523e-02 -1.03083951e-02  1.35244370e-01  1.28845826e-01\n",
      " -2.36540169e-01  5.34853265e-02 -4.98272106e-02  1.14121072e-01\n",
      "  6.98999241e-02  6.89669624e-02 -4.07548323e-02  3.21157761e-02\n",
      "  1.64505299e-02 -1.60979614e-01  8.39054659e-02 -1.55407220e-01\n",
      " -8.82212371e-02  1.14905341e-02  1.38943195e-01  2.16684770e-02\n",
      " -5.69336899e-02 -1.59323677e-01  1.31446749e-01  2.07527168e-02\n",
      "  2.57526964e-01  4.88353632e-02 -2.71650791e-01  4.32567224e-02\n",
      "  6.15708344e-02 -6.12813383e-02  9.88380285e-04  2.31557190e-02\n",
      "  5.37726097e-02  1.29450997e-02 -1.32688671e-01  3.90203157e-03\n",
      "  9.46867764e-02  5.82214445e-02 -1.55290663e-01  1.20237609e-02\n",
      "  1.85520388e-02  1.09892450e-02 -5.12328334e-02 -1.84201926e-01\n",
      " -1.11649930e-01  2.62355544e-02  1.94901794e-01  1.80554539e-02\n",
      "  2.23242536e-01  9.23385620e-02  1.49391787e-02  1.17243826e-01\n",
      " -5.60552329e-02 -5.30622192e-02 -2.56163366e-02 -1.22210003e-01\n",
      " -3.78365517e-02  2.35972051e-02 -1.81677509e-02 -1.12890474e-01\n",
      "  2.98198126e-02  1.08237974e-01  3.99199724e-02 -1.39470659e-02]\n"
     ]
    }
   ],
   "source": [
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1625202451520,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "wen70awiaTGu",
    "outputId": "1c791631-02ea-4f0f-cee0-08f627ab0cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is you name?\n",
      "[[0.787295]]\n",
      "what is your surname?\n",
      "[[0.74842185]]\n",
      "how old are you?\n",
      "[[0.9056794]]\n",
      "what is your age?\n",
      "[[0.7776394]]\n",
      "do you know a joke?\n",
      "[[0.7742292]]\n",
      "are you single?\n",
      "[[0.8627101]]\n",
      "are you married?\n",
      "[[0.8182191]]\n",
      "will you marry me?\n",
      "[[0.6899673]]\n",
      "do you like people?\n",
      "[[0.83937776]]\n",
      "are you part of the matrix?\n",
      "[[0.728569]]\n",
      "are you beautiful?\n",
      "[[0.8552867]]\n",
      "tell me about your personality?\n",
      "[[0.674126]]\n",
      "youâ€™re annoying?\n",
      "[[0.66352415]]\n",
      "i want to speak to a human?\n",
      "[[0.56407136]]\n",
      "can you speak spanish?\n",
      "[[0.7725482]]\n",
      "can you speak portuguese?\n",
      "[[0.74399805]]\n",
      "quantas linguas voce consegue falar?\n",
      "[[0.51650023]]\n",
      "how many languages can you speak?\n",
      "[[0.81488883]]\n",
      "what do you do with my data?\n",
      "[[0.7612737]]\n",
      "who made you?\n",
      "[[0.70288336]]\n",
      "where do you live?\n",
      "[[0.8211955]]\n",
      "whatâ€™s the weather like today?\n",
      "[[0.69491804]]\n",
      "do you have a job for me?\n",
      "[[0.68590546]]\n",
      "are you expensive?\n",
      "[[0.86247873]]\n",
      "do you get smarter?\n",
      "[[0.7897574]]\n",
      "can we talk later?\n",
      "[[0.69680667]]\n",
      "are you fine?\n",
      "[[0.84208554]]\n",
      "are you fine?\n",
      "[[0.8448594]]\n",
      "how is your day?\n",
      "[[0.78167486]]\n",
      "how is your day?\n",
      "[[0.7801584]]\n",
      "how is your day?\n",
      "[[0.81054616]]\n",
      "how is your day?\n",
      "[[0.7869029]]\n",
      "are you real?\n",
      "[[0.8751568]]\n",
      "are you a student?\n",
      "[[0.7849139]]\n",
      "what do you rhink abou life?\n",
      "[[0.71897805]]\n",
      "do you like it classes?\n",
      "[[0.7415234]]\n",
      "do you like it classes?\n",
      "[[0.728486]]\n",
      "what is your favorite food?\n",
      "[[0.7404168]]\n",
      "yes?\n",
      "[[0.7453557]]\n",
      "yes\n",
      "\n",
      "no?\n",
      "[[0.6572435]]\n",
      "no?\n",
      "[[0.6828134]]\n",
      "ok?\n",
      "[[0.73139864]]\n",
      "ok?\n",
      "[[0.7223352]]\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokens:\n",
    "    v2 = model.infer_vector(word_tokenize(i.lower()))\n",
    "    print(i)\n",
    "    print(cosine_similarity(v1.reshape(1, -1),v2.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1625202451520,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "cGEBbf8eaTGu"
   },
   "outputs": [],
   "source": [
    "def calc_prob(v1, q):\n",
    "    probs = dict()\n",
    "    for i in q:\n",
    "        v2 = model.infer_vector(word_tokenize(i.lower()))\n",
    "        sim = cosine_similarity(v1.reshape(1, -1),v2.reshape(1, -1))\n",
    "        #print(i)\n",
    "        #print(sim)\n",
    "        probs[i] = sim[0][0]\n",
    "    sorted_d = dict( sorted(probs.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "    return list(sorted_d.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1625202451520,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "NFb4-p0faTGu",
    "outputId": "0cd58c0a-2388-412a-c85c-a0b58d65ba1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how old are you?', 0.9053973)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_prob(v1, sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISG0dPL8aTGv"
   },
   "source": [
    "Since it appears that this model is more effective lets incorporate it into our chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "executionInfo": {
     "elapsed": 12433,
     "status": "error",
     "timestamp": 1625202463943,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "S8JaMj8TaTGv",
    "outputId": "32ea27c9-eb69-4808-b6bc-fc6c6bbef947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andi: My name is Andi. Do you want to chat? If you want to exit, type Bye!\n",
      "how old are you?\n",
      "Andi: I am 1 year old.\n",
      " (With similarity of  0.97205156 )\n",
      "what is your name?\n",
      "Andi: My name is Andi.\n",
      " (With similarity of  0.94889593 )\n",
      "What is your surname?\n",
      "Andi: AXL1.\n",
      " (With similarity of  0.9684694 )\n",
      "Can you speak portuguese?\n",
      "Andi: I am not human.\n",
      " (With similarity of  0.96559274 )\n",
      "Can we talk later?\n",
      "Andi: Yes.\n",
      " (With similarity of  0.96595126 )\n",
      "bye\n",
      "Andi: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Andi: My name is Andi. Do you want to chat? If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Andi: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Andi: \"+greeting(user_response))\n",
    "\n",
    "            else:\n",
    "                print(\"Andi: \",end=\"\")\n",
    "                resp= calc_prob(model.infer_vector(word_tokenize(user_response)), sent_tokens)\n",
    "                print(res[resp[0]], )\n",
    "                print(' (With similarity of ',resp[1],')')\n",
    "\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Andi: Bye! take care...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g55PaQhWputU"
   },
   "source": [
    "**After observing the performance** of both of the models there are some clear conclusions that we can draw from this:\n",
    "\n",
    "1. The Doc2vec model requires a lot more data for it to understand the relationship between words. And even after using a pre-trained model there is a clear lack of quality in the responses from the model compared to the cosine similarity model. \n",
    "2. The cosine similarity model works better for a smaller and more well defined dataset. This means that a few simple questions can be solved effectively but complex questions that require context wont be solved by it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1625201945306,
     "user": {
      "displayName": "Albert Anthony",
      "photoUrl": "",
      "userId": "06779463251817115427"
     },
     "user_tz": -420
    },
    "id": "C2siuK5KlRah"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook_Module_27_Hotel_Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
